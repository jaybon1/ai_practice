{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 경사 하강법 (에포크 5)\n",
      "시간 (running_time)                    : 0.016792699985672\n",
      "비용 (cost)                            : 731.305373045268425\n",
      "효율 (cost * running_time)             : 12.280591727458981\n",
      "\n",
      "확률적 경사 하강법 (에포크 5)\n",
      "시간 (running_time)                    : 15.871300600003451\n",
      "비용 (cost)                            : 0.002765406352634\n",
      "효율 (cost * running_time)             : 0.043890595503807\n",
      "\n",
      "미니 배치 경사 하강법 (에포크 5)\n",
      "시간 (running_time)                    : 0.182861100009177\n",
      "비용 (cost)                            : 0.321039128005671\n",
      "효율 (cost * running_time)             : 0.058705568093104\n",
      "\n",
      "미니 배치 랜덤 경사 하강법 (에포크 5)\n",
      "시간 (running_time)                    : 0.490814099990530\n",
      "비용 (cost)                            : 1.232032357737452\n",
      "효율 (cost * running_time)             : 0.604698852822118\n",
      "\n",
      "------------------------------------------------------------\n",
      "효율 순서\n",
      "0.043890595503807 ( 확률적 )\n",
      "0.058705568093104 ( 미니 배치 )\n",
      "0.604698852822118 ( 미니 배치 랜덤 )\n",
      "12.280591727458981 ( 배치 )\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 배치 경사 하강법\n",
    "def batch_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(X, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"배치\" , cost, theta)\n",
    "\n",
    "\n",
    "# 확률적 경사 하강법\n",
    "def stochastic_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m):\n",
    "            hypothesis = np.dot(X[j:j + 1, :], theta)\n",
    "            loss = hypothesis - y[j:j + 1]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + 1], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"확률적\" , cost, theta)\n",
    "\n",
    "# 미니 배치 경사 하강법\n",
    "def mini_batch_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m, batchSize):\n",
    "            hypothesis = np.dot(X[j:j + batchSize, :], theta)\n",
    "            loss = hypothesis - y[j:j + batchSize]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + batchSize], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"미니 배치\" , cost, theta)\n",
    "\n",
    "# 미니 배치 경사 하강법 + 미니 배치에서 데이터 30개를 랜덤하게 뽑아서 사용\n",
    "def mini_batch_random_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m, batchSize):\n",
    "            # 0~99까지의 중복되지 않는 랜덤한 정수 30개 생성\n",
    "            random_idx_list = np.random.choice(100, 30, replace=False)\n",
    "\n",
    "            # 배열에서 random_idx_list 안에 있는 인덱스만 추출\n",
    "            hypothesis = np.dot(X[j:j + batchSize, :][random_idx_list], theta)\n",
    "            loss = hypothesis - y[j:j + batchSize][random_idx_list]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + batchSize][:, random_idx_list], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"미니 배치 랜덤\" , cost, theta)\n",
    "\n",
    "# 경사하강법 속도 및 정확도 비교\n",
    "def checkSpeedAndEfficiency(func_list, X, y, theta, alpha, m, numIterations, batchSize):\n",
    "    test_list = []\n",
    "    for func in func_list:\n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        result = func(X, y, theta, alpha, m, numIterations, batchSize)\n",
    "        stop = timeit.default_timer()\n",
    "        \n",
    "        name, cost, th = result\n",
    "        running_time = stop - start\n",
    "        \n",
    "        cost_per_running_time = cost * running_time\n",
    "        print(name, f\"경사 하강법 (에포크 {numIterations})\")\n",
    "        print(\"시간 (running_time)                    :\", format(running_time, \".15f\"))\n",
    "        print(\"비용 (cost)                            :\", format(cost, \".15f\"))\n",
    "        print(\"효율 (cost * running_time)             :\", format(cost_per_running_time, \".15f\"))\n",
    "        print()\n",
    "        # test_list.append({\"name\": name, \"time\": running_time, \"cost\": cost, \"cost_per_time\": cost_per_running_time})\n",
    "        test_list.append({\"name\": name, \"cost_per_time\": cost_per_running_time, running_time: running_time, cost: cost})\n",
    "        test_list = sorted(test_list, key=lambda x: x[\"cost_per_time\"])\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"효율 순서\")\n",
    "    for item in test_list:\n",
    "        # print(f\"{count}.\" , item[\"name\"], \":\", format(item[\"cost_per_time\"], \".15f\"))\n",
    "        print( format(item[\"cost_per_time\"], \".15f\"), \"(\", item[\"name\"], \")\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# func_list = [batch_gradient_descent]\n",
    "\n",
    "# # 함수 리스트\n",
    "func_list = [batch_gradient_descent, \\\n",
    "            stochastic_gradient_descent, \\\n",
    "            mini_batch_gradient_descent, \\\n",
    "            mini_batch_random_gradient_descent]\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('data/byong_data_set1.csv')\n",
    "\n",
    "# 데이터 분리\n",
    "X = df[['height', 'weight']].values\n",
    "y = df[['bmi']].values\n",
    "m, n = np.shape(X) # m : 데이터 개수, n : 특성 개수\n",
    "numIterations = 5 # 에포크\n",
    "theta = np.ones(n).reshape(-1, 1) # 가중치\n",
    "alpha = 0.00001 # 학습률\n",
    "batchSize = 100 # 미니 배치 크기\n",
    "\n",
    "# 체크 \n",
    "checkSpeedAndEfficiency(func_list, X, y, theta, alpha, m, numIterations, 100)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
