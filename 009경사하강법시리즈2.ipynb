{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 경사 하강법 (에포크 6000)\n",
      "시간 (running_time)                    : 16.021210699982475\n",
      "비용 (cost)                            : 1.271536471392198\n",
      "효율 (cost * running_time)             : 20.371553720886645\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [56], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m batchSize \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m# 미니 배치 크기\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m# 체크 \u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m checkSpeedAndEfficiency(func_list, X, y, theta, alpha, m, numIterations, \u001b[39m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn [56], line 85\u001b[0m, in \u001b[0;36mcheckSpeedAndEfficiency\u001b[1;34m(func_list, X, y, theta, alpha, m, numIterations, batchSize)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m func_list:\n\u001b[0;32m     84\u001b[0m     start \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 85\u001b[0m     result \u001b[39m=\u001b[39m func(X, y, theta, alpha, m, numIterations, batchSize)\n\u001b[0;32m     86\u001b[0m     stop \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[0;32m     88\u001b[0m     name, cost, th \u001b[39m=\u001b[39m result\n",
      "Cell \u001b[1;32mIn [56], line 29\u001b[0m, in \u001b[0;36mstochastic_gradient_descent\u001b[1;34m(X, y, theta, alpha, m, numIterations, batchSize)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, numIterations):\n\u001b[0;32m     28\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, m):\n\u001b[1;32m---> 29\u001b[0m         hypothesis \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X[j:j \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, :], theta)\n\u001b[0;32m     30\u001b[0m         loss \u001b[39m=\u001b[39m hypothesis \u001b[39m-\u001b[39m y[j:j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     31\u001b[0m         \u001b[39m# avg cost per example (the 2 in 2*m doesn't really matter here.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[39m# But to be consistent with the gradient, I include it)\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# 배치 경사 하강법\n",
    "def batch_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(X, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"배치\" , cost, theta)\n",
    "\n",
    "\n",
    "# 확률적 경사 하강법\n",
    "def stochastic_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m):\n",
    "            hypothesis = np.dot(X[j:j + 1, :], theta)\n",
    "            loss = hypothesis - y[j:j + 1]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + 1], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"확률적\" , cost, theta)\n",
    "\n",
    "# 미니 배치 경사 하강법\n",
    "def mini_batch_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m, batchSize):\n",
    "            hypothesis = np.dot(X[j:j + batchSize, :], theta)\n",
    "            loss = hypothesis - y[j:j + batchSize]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + batchSize], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"미니 배치\" , cost, theta)\n",
    "\n",
    "# 미니 배치 경사 하강법 + 미니 배치에서 데이터 30개를 랜덤하게 뽑아서 사용\n",
    "def mini_batch_random_gradient_descent(X : np.ndarray, y : np.ndarray, theta, alpha, m, numIterations, batchSize):\n",
    "    xTrans = X.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        for j in range(0, m, batchSize):\n",
    "            # 0~99까지의 중복되지 않는 랜덤한 정수 30개 생성\n",
    "            random_idx_list = np.random.choice(100, 30, replace=False)\n",
    "\n",
    "            # 배열에서 random_idx_list 안에 있는 인덱스만 추출\n",
    "            hypothesis = np.dot(X[j:j + batchSize, :][random_idx_list], theta)\n",
    "            loss = hypothesis - y[j:j + batchSize][random_idx_list]\n",
    "            # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "            # But to be consistent with the gradient, I include it)\n",
    "            cost = np.sum(loss ** 2) / (2 * m)\n",
    "            # avg gradient per example\n",
    "            gradient = np.dot(xTrans[:, j:j + batchSize][:, random_idx_list], loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "    # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "    return (\"미니 배치 랜덤\" , cost, theta)\n",
    "\n",
    "# 경사하강법 속도 및 정확도 비교\n",
    "def checkSpeedAndEfficiency(func_list, X, y, theta, alpha, m, numIterations, batchSize):\n",
    "    test_list = []\n",
    "    for func in func_list:\n",
    "        \n",
    "        start = timeit.default_timer()\n",
    "        result = func(X, y, theta, alpha, m, numIterations, batchSize)\n",
    "        stop = timeit.default_timer()\n",
    "        \n",
    "        name, cost, th = result\n",
    "        running_time = stop - start\n",
    "        \n",
    "        cost_per_running_time = cost * running_time\n",
    "        print(name, f\"경사 하강법 (에포크 {numIterations})\")\n",
    "        print(\"시간 (running_time)                    :\", format(running_time, \".15f\"))\n",
    "        print(\"비용 (cost)                            :\", format(cost, \".15f\"))\n",
    "        print(\"효율 (cost * running_time)             :\", format(cost_per_running_time, \".15f\"))\n",
    "        print()\n",
    "        # test_list.append({\"name\": name, \"time\": running_time, \"cost\": cost, \"cost_per_time\": cost_per_running_time})\n",
    "        test_list.append({\"name\": name, \"cost_per_time\": cost_per_running_time, running_time: running_time, cost: cost})\n",
    "        test_list = sorted(test_list, key=lambda x: x[\"cost_per_time\"])\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"효율 순서\")\n",
    "    for item in test_list:\n",
    "        # print(f\"{count}.\" , item[\"name\"], \":\", format(item[\"cost_per_time\"], \".15f\"))\n",
    "        print( format(item[\"cost_per_time\"], \".15f\"), \"(\", item[\"name\"], \")\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# func_list = [batch_gradient_descent]\n",
    "\n",
    "# # 함수 리스트\n",
    "func_list = [batch_gradient_descent, \\\n",
    "            stochastic_gradient_descent, \\\n",
    "            mini_batch_gradient_descent, \\\n",
    "            mini_batch_random_gradient_descent]\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('data/byong_data_set1.csv')\n",
    "\n",
    "# 데이터 분리\n",
    "X = df[['height', 'weight']].values\n",
    "y = df[['bmi']].values\n",
    "m, n = np.shape(X) # m : 데이터 개수, n : 특성 개수\n",
    "numIterations = 5 # 에포크\n",
    "theta = np.ones(n).reshape(-1, 1) # 가중치\n",
    "alpha = 0.00001 # 학습률\n",
    "batchSize = 100 # 미니 배치 크기\n",
    "\n",
    "# 체크 \n",
    "checkSpeedAndEfficiency(func_list, X, y, theta, alpha, m, numIterations, 100)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
